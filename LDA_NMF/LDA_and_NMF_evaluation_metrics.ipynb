{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "silent-egyptian",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/rep/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Packages need for data pre-process\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "from scipy import sparse\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# word normalization (remove repeated)\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from repeatedReplacer import RepeatReplacer \n",
    "replacer = RepeatReplacer()\n",
    "\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# read in functions from 'preprocessingFunctions.py'\n",
    "import preprocessingFunctions \n",
    "\n",
    "# count time \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-border",
   "metadata": {},
   "source": [
    "# 1. Tweets from 4 distinct users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "wrapped-brief",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12780, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of         user_id  user_id_new      screen_name  \\\n",
       "0      27902825            2    UMichFootball   \n",
       "1      27902825            2    UMichFootball   \n",
       "2      27902825            2    UMichFootball   \n",
       "3      27902825            2    UMichFootball   \n",
       "4      27902825            2    UMichFootball   \n",
       "...         ...          ...              ...   \n",
       "12775  19071682            3  breakingweather   \n",
       "12776  19071682            3  breakingweather   \n",
       "12777  19071682            3  breakingweather   \n",
       "12778  19071682            3  breakingweather   \n",
       "12779  19071682            3  breakingweather   \n",
       "\n",
       "                                                    text  \n",
       "0                              ðŸ‘‡ https://t.co/swtsZWWaJe  \n",
       "1      Leave it all on the field! @UMichFootball! Bes...  \n",
       "2      Thereâ€™s no time to look backwardsâ€¦ only ahead!...  \n",
       "3         2ï¸âƒ£4ï¸âƒ£:0ï¸âƒ£0ï¸âƒ£:0ï¸âƒ£0ï¸âƒ£ â³ https://t.co/eM3yUXJXaq  \n",
       "4      Itâ€™s called â€œThe Gameâ€™ for a reason. \\r\\n\\r\\n#...  \n",
       "...                                                  ...  \n",
       "12775  A flash flood emergency is in effect for south...  \n",
       "12776  Now that Barry, the first hurricane to make U....  \n",
       "12777  Showers and locally heavy, drenching thunderst...  \n",
       "12778  While Monday felt like a typical summer day in...  \n",
       "12779  Residents from the northern Philippines to Tai...  \n",
       "\n",
       "[12780 rows x 4 columns]>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dataset\n",
    "import os \n",
    "os.chdir(\"/home/rep/scRNA-seq_clustering_to_Twitter/P1_preprocessing\")\n",
    "os.getcwd()\n",
    "four = pd.read_csv('four_users.csv')\n",
    "del four['Unnamed: 0']\n",
    "\n",
    "print(four.shape)\n",
    "four.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "revolutionary-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lowercase and convert to list\n",
    "data = four.text.str.lower().values.tolist()\n",
    "data = [preprocessingFunctions.preProcessingFcn(tweet) for tweet in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "excessive-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the tweets and remove punctuations\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "early-winter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "stop_words = stopwords.words('english')\n",
    "data_words_unigrams = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in data_words]\n",
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "otherwise-gabriel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "data = []\n",
    "for i in data_words_unigrams:\n",
    "    tweet = ' '.join(i)\n",
    "    data.append(tweet)\n",
    "\n",
    "data_stemming = [preprocessingFunctions.stemming(tweet) for tweet in data]\n",
    "\n",
    "data_stemming_temp = []\n",
    "for i in data_stemming:\n",
    "    alist = i.split()\n",
    "    data_stemming_temp.append(alist)\n",
    "    \n",
    "data_stemming = data_stemming_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "brown-heavy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of remaining tweets w.r.t. original tweets: 97.86%\n",
      "Proportion of removed tweets w.r.t. original tweets: 2.14%\n"
     ]
    }
   ],
   "source": [
    "# Remove 80% of the least frequent words\n",
    "words_dict, data_stemming1, empty_idx = preprocessingFunctions.trim_noise(data_stemming, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "varying-receptor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The lowest word frequency in the remaining tweets \n",
    "min(words_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "precise-citizenship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12507, 2043)\n",
      "       beatosu  best  colleg  field  footbal  goblu  leav  rivalri  \\\n",
      "1          1.0   1.0     1.0    1.0      1.0    1.0   1.0      1.0   \n",
      "2          1.0   0.0     0.0    0.0      0.0    1.0   0.0      0.0   \n",
      "4          1.0   0.0     0.0    0.0      0.0    1.0   0.0      0.0   \n",
      "5          1.0   0.0     0.0    0.0      0.0    1.0   0.0      0.0   \n",
      "7          1.0   0.0     0.0    0.0      0.0    1.0   0.0      0.0   \n",
      "...        ...   ...     ...    ...      ...    ...   ...      ...   \n",
      "12775      0.0   0.0     0.0    0.0      0.0    0.0   0.0      0.0   \n",
      "12776      0.0   0.0     0.0    0.0      0.0    0.0   0.0      0.0   \n",
      "12777      0.0   0.0     0.0    0.0      0.0    0.0   0.0      0.0   \n",
      "12778      0.0   0.0     0.0    0.0      0.0    0.0   0.0      0.0   \n",
      "12779      0.0   0.0     0.0    0.0      0.0    0.0   0.0      0.0   \n",
      "\n",
      "       umichfootbal  ahead  ...  antil  lesser  uptick  dorian  \\\n",
      "1               1.0    0.0  ...    0.0     0.0     0.0     0.0   \n",
      "2               1.0    1.0  ...    0.0     0.0     0.0     0.0   \n",
      "4               0.0    0.0  ...    0.0     0.0     0.0     0.0   \n",
      "5               0.0    0.0  ...    0.0     0.0     0.0     0.0   \n",
      "7               1.0    0.0  ...    0.0     0.0     0.0     0.0   \n",
      "...             ...    ...  ...    ...     ...     ...     ...   \n",
      "12775           0.0    0.0  ...    0.0     0.0     0.0     0.0   \n",
      "12776           0.0    0.0  ...    0.0     0.0     0.0     0.0   \n",
      "12777           0.0    0.0  ...    0.0     0.0     0.0     0.0   \n",
      "12778           0.0    0.0  ...    0.0     0.0     0.0     0.0   \n",
      "12779           0.0    0.0  ...    0.0     0.0     0.0     0.0   \n",
      "\n",
      "       hurricanedorian  bailu  krosa  lekima  flossi  barri  \n",
      "1                  0.0    0.0    0.0     0.0     0.0    0.0  \n",
      "2                  0.0    0.0    0.0     0.0     0.0    0.0  \n",
      "4                  0.0    0.0    0.0     0.0     0.0    0.0  \n",
      "5                  0.0    0.0    0.0     0.0     0.0    0.0  \n",
      "7                  0.0    0.0    0.0     0.0     0.0    0.0  \n",
      "...                ...    ...    ...     ...     ...    ...  \n",
      "12775              0.0    0.0    0.0     0.0     0.0    0.0  \n",
      "12776              0.0    0.0    0.0     0.0     0.0    1.0  \n",
      "12777              0.0    0.0    0.0     0.0     0.0    1.0  \n",
      "12778              0.0    0.0    0.0     0.0     0.0    1.0  \n",
      "12779              0.0    0.0    0.0     0.0     0.0    0.0  \n",
      "\n",
      "[12507 rows x 2043 columns]\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "##### Create document-term matrix #####\n",
    "#######################################\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_stemming1)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_stemming1\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "a_s = gensim.matutils.corpus2dense(corpus, num_terms = len(words_dict))\n",
    "b_s = a_s.T.astype(np.float64)\n",
    "\n",
    "# Extract Document index\n",
    "selected_idex = [x for x in list(four.index) if x not in empty_idx]\n",
    "\n",
    "# Obtain remaining terms\n",
    "words = [] \n",
    "for i,j in enumerate(id2word):\n",
    "    words.append(id2word[i])\n",
    "\n",
    "# Create a dataframe for the document-term matrix\n",
    "b_ss = pd.DataFrame(b_s, columns=words, index=selected_idex)\n",
    "print(b_ss.shape)\n",
    "print(b_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fantastic-choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain attributes for the remaining tweet \n",
    "four_after = four.drop(empty_idx, axis=0)\n",
    "\n",
    "tweets_processed = []\n",
    "for i in data_stemming1:\n",
    "    tweet = ' '.join(i)\n",
    "    tweets_processed.append(tweet)\n",
    "\n",
    "four_after['tweets_processed'] = list(tweets_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-assessment",
   "metadata": {},
   "source": [
    "## four_LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-stand",
   "metadata": {},
   "source": [
    "#### - num of cluster = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "hollow-miracle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 10.922403335571289 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "lda_model = gensim.models.ldamodel.LdaModel(passes=10,corpus=corpus,id2word=id2word,num_topics = 4, random_state = 44)\n",
    "print(\"--- %s seconds ---\" % (time. time() - start_time))\n",
    "\n",
    "df_doc_topic = pd.DataFrame(columns = ['Dominant_Topic'])\n",
    "for i in range(len(corpus)):\n",
    "    a = lda_model.get_document_topics(corpus[i])\n",
    "    a.sort(key = lambda x: x[1], reverse=True) #highest prob topic first\n",
    "    row = [a[0][0]]\n",
    "    df_doc_topic.loc[i] = row\n",
    "pred_LDA = list(df_doc_topic[\"Dominant_Topic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "current-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_four_pred = pd.DataFrame({'LDA_four_pred': pred_LDA})\n",
    "# LDA_four_pred.to_csv(\"/home/rep/scRNA-seq_clustering_to_Twitter/P2_scRNAseq_LDA_NMF/LDA_NMF/LDA_NMF_files/LDA_four_pred_contingency.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-adapter",
   "metadata": {},
   "source": [
    "#### - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "apart-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "True_Label = list(four_after[\"user_id_new\"])\n",
    "Correct_target = pd.DataFrame(four_after[[\"screen_name\", \"user_id_new\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "clinical-nickname",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_purity = []\n",
    "LDA_AMI = []\n",
    "LDA_ARI = []\n",
    "\n",
    "for i in range(0,68):\n",
    "    num_cluster = i+3\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(passes=10,corpus=corpus,id2word=id2word,num_topics = num_cluster, random_state = 44)\n",
    "    df_doc_topic = pd.DataFrame(columns = ['Dominant_Topic'])\n",
    "    for i in range(len(corpus)):\n",
    "        a = lda_model.get_document_topics(corpus[i])\n",
    "        a.sort(key = lambda x: x[1], reverse=True) #highest prob topic first\n",
    "        row = [a[0][0]]\n",
    "        df_doc_topic.loc[i] = row\n",
    "    pred_LDA = list(df_doc_topic[\"Dominant_Topic\"])\n",
    "    \n",
    "    # purity\n",
    "    df_compare = pd.concat([df_doc_topic, Correct_target.reindex(df_doc_topic.index)], axis=1)\n",
    "    nominator = df_compare.groupby([\"Dominant_Topic\", \"user_id_new\"], as_index=False)['screen_name'].count().sort_values('screen_name', ascending=False).drop_duplicates('Dominant_Topic')[\"screen_name\"].sum()\n",
    "    purity = nominator/len(True_Label)\n",
    "    LDA_purity.append(purity)\n",
    "    \n",
    "    # AMI\n",
    "    AMI = adjusted_mutual_info_score(True_Label, pred_LDA)\n",
    "    LDA_AMI.append(AMI)\n",
    "    \n",
    "    # ARI\n",
    "    ARI = adjusted_rand_score(True_Label, pred_LDA)\n",
    "    LDA_ARI.append(ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "private-panama",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cluster = []\n",
    "for i in range(0,68):\n",
    "    num_cluster.append(i+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bound-prague",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_cluster</th>\n",
       "      <th>Purity</th>\n",
       "      <th>AMI</th>\n",
       "      <th>ARI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.587271</td>\n",
       "      <td>0.536204</td>\n",
       "      <td>0.484470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.880307</td>\n",
       "      <td>0.840129</td>\n",
       "      <td>0.880648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.854242</td>\n",
       "      <td>0.737149</td>\n",
       "      <td>0.754559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.840010</td>\n",
       "      <td>0.717206</td>\n",
       "      <td>0.766720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.794915</td>\n",
       "      <td>0.634953</td>\n",
       "      <td>0.676926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>66</td>\n",
       "      <td>0.629168</td>\n",
       "      <td>0.212606</td>\n",
       "      <td>0.099605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>67</td>\n",
       "      <td>0.635164</td>\n",
       "      <td>0.217742</td>\n",
       "      <td>0.127718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>68</td>\n",
       "      <td>0.638842</td>\n",
       "      <td>0.212062</td>\n",
       "      <td>0.103063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>69</td>\n",
       "      <td>0.644279</td>\n",
       "      <td>0.225408</td>\n",
       "      <td>0.123529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>70</td>\n",
       "      <td>0.640122</td>\n",
       "      <td>0.213330</td>\n",
       "      <td>0.087317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_cluster    Purity       AMI       ARI\n",
       "0             3  0.587271  0.536204  0.484470\n",
       "1             4  0.880307  0.840129  0.880648\n",
       "2             5  0.854242  0.737149  0.754559\n",
       "3             6  0.840010  0.717206  0.766720\n",
       "4             7  0.794915  0.634953  0.676926\n",
       "..          ...       ...       ...       ...\n",
       "63           66  0.629168  0.212606  0.099605\n",
       "64           67  0.635164  0.217742  0.127718\n",
       "65           68  0.638842  0.212062  0.103063\n",
       "66           69  0.644279  0.225408  0.123529\n",
       "67           70  0.640122  0.213330  0.087317\n",
       "\n",
       "[68 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA_evaluation_metrics = {'num_cluster': num_cluster,'Purity': LDA_purity, 'AMI': LDA_AMI, 'ARI': LDA_ARI}\n",
    "LDA_evaluation_metrics = pd.DataFrame(data = LDA_evaluation_metrics)\n",
    "LDA_evaluation_metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "specialized-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA_evaluation_metrics.to_csv(\"/home/rep/scRNA-seq_clustering_to_Twitter/P2_scRNAseq_LDA_NMF/LDA_NMF/LDA_NMF_files/LDA_evaluation_metrics_four.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-history",
   "metadata": {},
   "source": [
    "## four_NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-worcester",
   "metadata": {},
   "source": [
    "#### - num of cluster = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "increased-visitor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.3957340717315674 seconds ---\n"
     ]
    }
   ],
   "source": [
    "texts = four_after['tweets_processed']\n",
    "start_time = time.time()\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform(texts)\n",
    "nmf = NMF(n_components=4, random_state=44).fit(tfidf)\n",
    "nmf_output = nmf.fit_transform(tfidf)\n",
    "print(\"--- %s seconds ---\" % (time. time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "weekly-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_topics(vectorizer=tfidf_vectorizer, lda_model=nmf, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "topic_keywords = show_topics(vectorizer=tfidf_vectorizer, lda_model=nmf, n_words=20)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "adapted-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_labels = []\n",
    "for i in nmf_output:\n",
    "    i = list(i)\n",
    "    index = i.index(max(i))\n",
    "    estimated_labels.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "binding-imaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "NMF_four_pred = pd.DataFrame({'NMF_four_pred': estimated_labels})\n",
    "# NMF_four_pred.to_csv(\"/home/rep/scRNA-seq_clustering_to_Twitter/P2_scRNAseq_LDA_NMF/LDA_NMF/LDA_NMF_files/NMF_four_pred_contingency.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-romania",
   "metadata": {},
   "source": [
    "#### - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "unavailable-pride",
   "metadata": {},
   "outputs": [],
   "source": [
    "NMF_purity = []\n",
    "NMF_AMI = []\n",
    "NMF_ARI = []\n",
    "\n",
    "for i in range(0,68):\n",
    "    num_cluster = i+3\n",
    "    \n",
    "    nmf = NMF(n_components=num_cluster, random_state=44).fit(tfidf)\n",
    "    nmf_output = nmf.fit_transform(tfidf)\n",
    "    \n",
    "    estimate_NMF = []\n",
    "    for j in nmf_output:\n",
    "        j = list(j)\n",
    "        index = j.index(max(j))\n",
    "        estimate_NMF.append(index)\n",
    "    \n",
    "    # purity\n",
    "    estimate_NMF_matrix = pd.DataFrame({'estimate_NMF': estimate_NMF})\n",
    "    df_compare = pd.concat([estimate_NMF_matrix, Correct_target], axis=1)\n",
    "    numerator = df_compare.groupby(['estimate_NMF', \"user_id_new\"], as_index=False)['screen_name'].count().sort_values('screen_name', ascending=False).drop_duplicates('estimate_NMF')[\"screen_name\"].sum()\n",
    "    purity = numerator/len(True_Label)\n",
    "    NMF_purity.append(purity)\n",
    "    \n",
    "    # AMI\n",
    "    AMI = adjusted_mutual_info_score(True_Label, estimate_NMF)\n",
    "    NMF_AMI.append(AMI)\n",
    "    \n",
    "    # ARI\n",
    "    ARI = adjusted_rand_score(True_Label, estimate_NMF)\n",
    "    NMF_ARI.append(ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "taken-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cluster = []\n",
    "for i in range(0,68):\n",
    "    num_cluster.append(i+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "subsequent-imperial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_cluster</th>\n",
       "      <th>Purity</th>\n",
       "      <th>AMI</th>\n",
       "      <th>ARI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.660590</td>\n",
       "      <td>0.686622</td>\n",
       "      <td>0.549704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.856960</td>\n",
       "      <td>0.795673</td>\n",
       "      <td>0.817598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.855361</td>\n",
       "      <td>0.757928</td>\n",
       "      <td>0.770512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.863117</td>\n",
       "      <td>0.733888</td>\n",
       "      <td>0.709325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.842088</td>\n",
       "      <td>0.673998</td>\n",
       "      <td>0.637351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>66</td>\n",
       "      <td>0.764612</td>\n",
       "      <td>0.339966</td>\n",
       "      <td>0.079186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>67</td>\n",
       "      <td>0.769649</td>\n",
       "      <td>0.344987</td>\n",
       "      <td>0.079101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>68</td>\n",
       "      <td>0.755977</td>\n",
       "      <td>0.335434</td>\n",
       "      <td>0.075754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>69</td>\n",
       "      <td>0.757736</td>\n",
       "      <td>0.333756</td>\n",
       "      <td>0.074803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>70</td>\n",
       "      <td>0.772847</td>\n",
       "      <td>0.344642</td>\n",
       "      <td>0.076900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_cluster    Purity       AMI       ARI\n",
       "0             3  0.660590  0.686622  0.549704\n",
       "1             4  0.856960  0.795673  0.817598\n",
       "2             5  0.855361  0.757928  0.770512\n",
       "3             6  0.863117  0.733888  0.709325\n",
       "4             7  0.842088  0.673998  0.637351\n",
       "..          ...       ...       ...       ...\n",
       "63           66  0.764612  0.339966  0.079186\n",
       "64           67  0.769649  0.344987  0.079101\n",
       "65           68  0.755977  0.335434  0.075754\n",
       "66           69  0.757736  0.333756  0.074803\n",
       "67           70  0.772847  0.344642  0.076900\n",
       "\n",
       "[68 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NMF_evaluation_metrics = {'num_cluster': num_cluster,'Purity': NMF_purity, 'AMI': NMF_AMI, 'ARI': NMF_ARI}\n",
    "NMF_evaluation_metrics = pd.DataFrame(data = NMF_evaluation_metrics)\n",
    "NMF_evaluation_metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "altered-patrick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF_evaluation_metrics.to_csv(\"/home/rep/scRNA-seq_clustering_to_Twitter/P2_scRNAseq_LDA_NMF/LDA_NMF/LDA_NMF_files/NMF_evaluation_metrics_four.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-boston",
   "metadata": {},
   "source": [
    "# 2. \"jobs\" Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "unique-bulgaria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27900, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                       time                                               text  \\\n",
       "0      2009-08-01 10:25:36  Now Hiring:  Storage Architect II http://bit.l...   \n",
       "1      2009-08-01 22:57:06  \"The Steve Jobs method\" discussion on Hacker N...   \n",
       "2      2009-08-01 23:27:08  AZ Jobs | Taco Bell Restaurant General Manager...   \n",
       "3      2009-08-01 09:55:12  TN Jobs | SLP Travel Job in Knoxville Area, TN...   \n",
       "4      2009-08-01 05:58:39  NJ Jobs | New Jersey Travel or Perm job- OT at...   \n",
       "...                    ...                                                ...   \n",
       "27895  2009-11-01 02:15:14  these guys have to wake up. make him work alre...   \n",
       "27896  2009-11-01 03:04:26  Therapy Jobs at HCR! Physical Therapist / PT -...   \n",
       "27897  2009-11-01 00:21:24              hospitality jobs http://bit.ly/3XvUT1   \n",
       "27898  2009-11-01 03:26:41  Obama Tempers Economic News With Caution On Jo...   \n",
       "27899  2009-11-01 03:21:23  EXCITING, getting ready for my 1st job test =D...   \n",
       "\n",
       "                   sn        date  \n",
       "0       ChicagoJobAds  2009-08-01  \n",
       "1              hnshah  2009-08-01  \n",
       "2          ZuluJobsAZ  2009-08-01  \n",
       "3          ZuluJobsTN  2009-08-01  \n",
       "4          ZuluJobsNJ  2009-08-01  \n",
       "...               ...         ...  \n",
       "27895     yankee32879  2009-11-01  \n",
       "27896      lydsterj2w  2009-11-01  \n",
       "27897  Ur_WebInfoNews  2009-11-01  \n",
       "27898  suzanne_newton  2009-11-01  \n",
       "27899  NadaAbdulrazak  2009-11-01  \n",
       "\n",
       "[27900 rows x 4 columns]>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dataset\n",
    "jobs = pd.read_csv(\"jobs_tweets_sampled_three_month.csv\")\n",
    "del jobs['Unnamed: 0']\n",
    "\n",
    "print(jobs.shape)\n",
    "jobs.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "stupid-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lowercase and convert to list\n",
    "data = jobs.text.str.lower().values.tolist()\n",
    "data = [preprocessingFunctions.preProcessingFcn(tweet) for tweet in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "informational-kenya",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the tweets and remove punctuations\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "presidential-resident",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "stop_words = stopwords.words('english')\n",
    "data_words_unigrams = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in data_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "decimal-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "data = []\n",
    "for i in data_words_unigrams:\n",
    "    tweet = ' '.join(i)\n",
    "    data.append(tweet)\n",
    "\n",
    "data_stemming = [preprocessingFunctions.stemming(tweet) for tweet in data]\n",
    "\n",
    "data_stemming_temp = []\n",
    "for i in data_stemming:\n",
    "    alist = i.split()\n",
    "    data_stemming_temp.append(alist)\n",
    "    \n",
    "data_stemming = data_stemming_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "basic-receiver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of remaining tweets w.r.t. original tweets: 99.99%\n",
      "Proportion of removed tweets w.r.t. original tweets: 0.01%\n"
     ]
    }
   ],
   "source": [
    "# Remove 90% of the least frequent words\n",
    "words_dict, data_stemming1, empty_idx1 = preprocessingFunctions.trim_noise(data_stemming, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "recovered-metro",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The lowest word frequency in the remaining tweets \n",
    "min(words_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "accompanied-immigration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282    http://bit.ly/rXYm5 :: e_jobs: &#10148;Concurs...\n",
      "Name: text, dtype: object\n",
      "13865    legitimate_telecommute_jobs  http://bit.ly/16tkOq\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print the removed tweets \n",
    "for i in empty_idx1:\n",
    "    print(jobs.iloc[[i]].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "checked-probability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27898, 2136)\n",
      "       architect  hire   ii  job  discuss  news  steve  via   az  azjob  ...  \\\n",
      "0            1.0   1.0  1.0  1.0      0.0   0.0    0.0  0.0  0.0    0.0  ...   \n",
      "1            0.0   0.0  0.0  1.0      1.0   1.0    1.0  1.0  0.0    0.0  ...   \n",
      "2            0.0   1.0  0.0  2.0      0.0   0.0    0.0  0.0  2.0    1.0  ...   \n",
      "3            0.0   1.0  0.0  3.0      0.0   0.0    0.0  0.0  0.0    0.0  ...   \n",
      "4            0.0   1.0  0.0  3.0      0.0   0.0    0.0  0.0  0.0    0.0  ...   \n",
      "...          ...   ...  ...  ...      ...   ...    ...  ...  ...    ...  ...   \n",
      "27895        0.0   0.0  0.0  1.0      0.0   0.0    0.0  0.0  0.0    0.0  ...   \n",
      "27896        0.0   0.0  0.0  2.0      0.0   0.0    0.0  0.0  0.0    0.0  ...   \n",
      "27897        0.0   0.0  0.0  1.0      0.0   0.0    0.0  0.0  0.0    0.0  ...   \n",
      "27898        0.0   0.0  0.0  1.0      0.0   2.0    0.0  0.0  0.0    0.0  ...   \n",
      "27899        0.0   0.0  0.0  2.0      0.0   0.0    0.0  0.0  0.0    0.0  ...   \n",
      "\n",
      "       pogu  decis  airway  halloween  nano  effort  oct  iwow  persist  \\\n",
      "0       0.0    0.0     0.0        0.0   0.0     0.0  0.0   0.0      0.0   \n",
      "1       0.0    0.0     0.0        0.0   0.0     0.0  0.0   0.0      0.0   \n",
      "2       0.0    0.0     0.0        0.0   0.0     0.0  0.0   0.0      0.0   \n",
      "3       0.0    0.0     0.0        0.0   0.0     0.0  0.0   0.0      0.0   \n",
      "4       0.0    0.0     0.0        0.0   0.0     0.0  0.0   0.0      0.0   \n",
      "...     ...    ...     ...        ...   ...     ...  ...   ...      ...   \n",
      "27895   0.0    0.0     0.0        0.0   0.0     0.0  0.0   0.0      0.0   \n",
      "27896   0.0    0.0     0.0        0.0   0.0     0.0  0.0   0.0      0.0   \n",
      "27897   0.0    0.0     0.0        0.0   0.0     0.0  0.0   0.0      0.0   \n",
      "27898   0.0    0.0     0.0        0.0   0.0     0.0  0.0   0.0      0.0   \n",
      "27899   0.0    0.0     0.0        0.0   0.0     0.0  0.0   0.0      0.0   \n",
      "\n",
      "       overst  \n",
      "0         0.0  \n",
      "1         0.0  \n",
      "2         0.0  \n",
      "3         0.0  \n",
      "4         0.0  \n",
      "...       ...  \n",
      "27895     0.0  \n",
      "27896     0.0  \n",
      "27897     0.0  \n",
      "27898     0.0  \n",
      "27899     0.0  \n",
      "\n",
      "[27898 rows x 2136 columns]\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "##### Create document-term matrix #####\n",
    "#######################################\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_stemming1)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_stemming1\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "a_s = gensim.matutils.corpus2dense(corpus, num_terms = len(words_dict))\n",
    "b_s = a_s.T.astype(np.float64)\n",
    "\n",
    "# Extract Document index\n",
    "selected_idex = [x for x in list(jobs.index) if x not in empty_idx1]\n",
    "\n",
    "# Obtain remaining terms\n",
    "words = [] \n",
    "for i,j in enumerate(id2word):\n",
    "    words.append(id2word[i])\n",
    "\n",
    "# Create a dataframe\n",
    "b_ss = pd.DataFrame(b_s, columns=words, index=selected_idex)\n",
    "print(b_ss.shape)\n",
    "print(b_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "special-walter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of remaining tweets w.r.t. original tweets: 99.45%\n",
      "Proportion of removed tweets w.r.t. original tweets: 0.55%\n",
      "(27744, 2135)\n"
     ]
    }
   ],
   "source": [
    "# Remove the words that appear in >= 80% of the tweets\n",
    "word_dict, data_stemming2, b_ss_f, empty_idx2 = preprocessingFunctions.trim_common(b_ss, 80, data_stemming1)\n",
    "print(b_ss_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "protected-carbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the idex of all empty tweets after pre-processing\n",
    "empty_idx = empty_idx1 + empty_idx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "precious-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_after = jobs.drop(empty_idx, axis=0)\n",
    "\n",
    "tweets_processed = []\n",
    "for i in data_stemming2:\n",
    "    tweet = ' '.join(i)\n",
    "    tweets_processed.append(tweet)\n",
    "\n",
    "jobs_after['tweets_processed'] = list(tweets_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-produce",
   "metadata": {},
   "source": [
    "## jobs_LDA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-turning",
   "metadata": {},
   "source": [
    "#### - num of cluster = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "quantitative-digit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new Dictionary\n",
    "id2word = corpora.Dictionary(data_stemming2)\n",
    "\n",
    "# Create new Corpus\n",
    "texts = data_stemming2\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "stupid-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(passes=10,corpus=corpus,id2word=id2word,num_topics = 5, random_state = 44)\n",
    "df_doc_topic = pd.DataFrame(columns = ['Dominant_Topic'])\n",
    "for i in range(len(corpus)):\n",
    "    a = lda_model.get_document_topics(corpus[i])\n",
    "    a.sort(key = lambda x: x[1], reverse=True) #highest prob topic first\n",
    "    row = [a[0][0]]\n",
    "    df_doc_topic.loc[i] = row\n",
    "pred_LDA = list(df_doc_topic[\"Dominant_Topic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "unique-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_jobs_pred = pd.DataFrame({'LDA_jobs_pred': pred_LDA})\n",
    "# LDA_jobs_pred.to_csv(\"/home/rep/scRNA-seq_clustering_to_Twitter/P2_scRNAseq_LDA_NMF/LDA_NMF/LDA_NMF_files/LDA_jobs_pred_contingency.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-discrimination",
   "metadata": {},
   "source": [
    "#### - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "leading-receptor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27744, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                       time                                               text  \\\n",
       "0      2009-08-01 10:25:36  Now Hiring:  Storage Architect II http://bit.l...   \n",
       "1      2009-08-01 22:57:06  \"The Steve Jobs method\" discussion on Hacker N...   \n",
       "2      2009-08-01 23:27:08  AZ Jobs | Taco Bell Restaurant General Manager...   \n",
       "3      2009-08-01 09:55:12  TN Jobs | SLP Travel Job in Knoxville Area, TN...   \n",
       "4      2009-08-01 05:58:39  NJ Jobs | New Jersey Travel or Perm job- OT at...   \n",
       "...                    ...                                                ...   \n",
       "27739  2009-11-01 02:15:14  these guys have to wake up. make him work alre...   \n",
       "27740  2009-11-01 03:04:26  Therapy Jobs at HCR! Physical Therapist / PT -...   \n",
       "27741  2009-11-01 00:21:24              hospitality jobs http://bit.ly/3XvUT1   \n",
       "27742  2009-11-01 03:26:41  Obama Tempers Economic News With Caution On Jo...   \n",
       "27743  2009-11-01 03:21:23  EXCITING, getting ready for my 1st job test =D...   \n",
       "\n",
       "                   sn        date  \\\n",
       "0       ChicagoJobAds  2009-08-01   \n",
       "1              hnshah  2009-08-01   \n",
       "2          ZuluJobsAZ  2009-08-01   \n",
       "3          ZuluJobsTN  2009-08-01   \n",
       "4          ZuluJobsNJ  2009-08-01   \n",
       "...               ...         ...   \n",
       "27739     yankee32879  2009-11-01   \n",
       "27740      lydsterj2w  2009-11-01   \n",
       "27741  Ur_WebInfoNews  2009-11-01   \n",
       "27742  suzanne_newton  2009-11-01   \n",
       "27743  NadaAbdulrazak  2009-11-01   \n",
       "\n",
       "                                        tweets_processed       category  label  \n",
       "0                                      hire architect ii  Advertisement      2  \n",
       "1                                 steve discuss news via           Junk      1  \n",
       "2      az taco bell restaur gener manag taco bell peo...  Advertisement      2  \n",
       "3      tn slp travel knoxvil area tn school system so...  Advertisement      2  \n",
       "4      nj new jersey travel perm ot sunbelt staf nj h...  Advertisement      2  \n",
       "...                                                  ...            ...    ...  \n",
       "27739                              guy make work alreadi       Personal      5  \n",
       "27740          therapi hcr physic therapist pt prn oh us          Other      4  \n",
       "27741                                             hospit  News/Politics      3  \n",
       "27742        obama temper econom news caution money news  News/Politics      3  \n",
       "27743                  excit get readi st test one dream       Personal      5  \n",
       "\n",
       "[27744 rows x 7 columns]>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_after = pd.read_csv(\"/home/rep/scRNA-seq_clustering_to_Twitter/P3_Proposed_workflow/PW_files/doc_metadata_stemming_jobs_hclabeled.csv\")\n",
    "del jobs_after['Unnamed: 0']\n",
    "\n",
    "print(jobs_after.shape)\n",
    "jobs_after.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "systematic-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "True_Label = list(jobs_after[\"category\"])\n",
    "Correct_target = pd.DataFrame(jobs_after[[\"category\", \"label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "separated-reflection",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_purity = []\n",
    "LDA_AMI = []\n",
    "LDA_ARI = []\n",
    "\n",
    "for i in range(0,68):\n",
    "    num_cluster = i+3\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(passes=10,corpus=corpus,id2word=id2word,num_topics = num_cluster, random_state = 44)\n",
    "    df_doc_topic = pd.DataFrame(columns = ['Dominant_Topic'])\n",
    "    for i in range(len(corpus)):\n",
    "        a = lda_model.get_document_topics(corpus[i])\n",
    "        a.sort(key = lambda x: x[1], reverse=True) #highest prob topic first\n",
    "        row = [a[0][0]]\n",
    "        df_doc_topic.loc[i] = row\n",
    "    pred_LDA = list(df_doc_topic[\"Dominant_Topic\"])\n",
    "    \n",
    "    # purity\n",
    "    df_compare = pd.concat([df_doc_topic, Correct_target.reindex(df_doc_topic.index)], axis=1)\n",
    "    nominator = df_compare.groupby([\"Dominant_Topic\", \"label\"], as_index=False)['category'].count().sort_values('category', ascending=False).drop_duplicates('Dominant_Topic')[\"category\"].sum()\n",
    "    purity = nominator/len(True_Label)\n",
    "    LDA_purity.append(purity)\n",
    "    \n",
    "    # AMI\n",
    "    AMI = adjusted_mutual_info_score(True_Label, pred_LDA)\n",
    "    LDA_AMI.append(AMI)\n",
    "    \n",
    "    # ARI\n",
    "    ARI = adjusted_rand_score(True_Label, pred_LDA)\n",
    "    LDA_ARI.append(ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "novel-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cluster = []\n",
    "for i in range(0,68):\n",
    "    num_cluster.append(i+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "expired-adams",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_cluster</th>\n",
       "      <th>Purity</th>\n",
       "      <th>AMI</th>\n",
       "      <th>ARI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.593137</td>\n",
       "      <td>0.273284</td>\n",
       "      <td>0.175242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.593137</td>\n",
       "      <td>0.224182</td>\n",
       "      <td>0.176947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.559436</td>\n",
       "      <td>0.166230</td>\n",
       "      <td>0.123447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.554606</td>\n",
       "      <td>0.160758</td>\n",
       "      <td>0.129027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.588091</td>\n",
       "      <td>0.166305</td>\n",
       "      <td>0.110241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>66</td>\n",
       "      <td>0.582829</td>\n",
       "      <td>0.100485</td>\n",
       "      <td>0.016178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>67</td>\n",
       "      <td>0.582108</td>\n",
       "      <td>0.101770</td>\n",
       "      <td>0.017655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>68</td>\n",
       "      <td>0.589028</td>\n",
       "      <td>0.107685</td>\n",
       "      <td>0.019478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>69</td>\n",
       "      <td>0.604924</td>\n",
       "      <td>0.112639</td>\n",
       "      <td>0.018889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>70</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.105958</td>\n",
       "      <td>0.018944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_cluster    Purity       AMI       ARI\n",
       "0             3  0.593137  0.273284  0.175242\n",
       "1             4  0.593137  0.224182  0.176947\n",
       "2             5  0.559436  0.166230  0.123447\n",
       "3             6  0.554606  0.160758  0.129027\n",
       "4             7  0.588091  0.166305  0.110241\n",
       "..          ...       ...       ...       ...\n",
       "63           66  0.582829  0.100485  0.016178\n",
       "64           67  0.582108  0.101770  0.017655\n",
       "65           68  0.589028  0.107685  0.019478\n",
       "66           69  0.604924  0.112639  0.018889\n",
       "67           70  0.588235  0.105958  0.018944\n",
       "\n",
       "[68 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA_evaluation_metrics = {'num_cluster': num_cluster,'Purity': LDA_purity, 'AMI': LDA_AMI, 'ARI': LDA_ARI}\n",
    "LDA_evaluation_metrics = pd.DataFrame(data = LDA_evaluation_metrics)\n",
    "LDA_evaluation_metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "about-balloon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA_evaluation_metrics.to_csv(\"/home/rep/scRNA-seq_clustering_to_Twitter/P2_scRNAseq_LDA_NMF/LDA_NMF/LDA_NMF_files/LDA_jobs_evaluation_metrics_raw_stemming.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-journey",
   "metadata": {},
   "source": [
    "## jobs_NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-confirmation",
   "metadata": {},
   "source": [
    "#### - num of cluster = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "retired-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = jobs_after['tweets_processed']\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform(texts)\n",
    "nmf = NMF(n_components=5, random_state=44).fit(tfidf)\n",
    "nmf_output = nmf.fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "armed-knitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_topics(vectorizer=tfidf_vectorizer, lda_model=nmf, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "topic_keywords = show_topics(vectorizer=tfidf_vectorizer, lda_model=nmf, n_words=20)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "popular-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_labels = []\n",
    "for i in nmf_output:\n",
    "    i = list(i)\n",
    "    index = i.index(max(i))\n",
    "    estimated_labels.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "introductory-johns",
   "metadata": {},
   "outputs": [],
   "source": [
    "NMF_jobs_pred = pd.DataFrame({'NMF_jobs_pred': estimated_labels})\n",
    "# NMF_jobs_pred.to_csv(\"/home/rep/scRNA-seq_clustering_to_Twitter/P2_scRNAseq_LDA_NMF/LDA_NMF/LDA_NMF_files/NMF_jobs_pred_contingency.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-assignment",
   "metadata": {},
   "source": [
    "#### - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "corrected-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "NMF_purity = []\n",
    "NMF_AMI = []\n",
    "NMF_ARI = []\n",
    "\n",
    "for i in range(0,68):\n",
    "    num_cluster = i+3\n",
    "    \n",
    "    nmf = NMF(n_components=num_cluster, random_state=44).fit(tfidf)\n",
    "    nmf_output = nmf.fit_transform(tfidf)\n",
    "    \n",
    "    estimate_NMF = []\n",
    "    for j in nmf_output:\n",
    "        j = list(j)\n",
    "        index = j.index(max(j))\n",
    "        estimate_NMF.append(index)\n",
    "    \n",
    "    # purity\n",
    "    estimate_NMF_matrix = pd.DataFrame({'estimate_NMF': estimate_NMF})\n",
    "    df_compare = pd.concat([estimate_NMF_matrix, Correct_target], axis=1)\n",
    "    numerator = df_compare.groupby(['estimate_NMF', \"label\"], as_index=False)['category'].count().sort_values('category', ascending=False).drop_duplicates('estimate_NMF')[\"category\"].sum()\n",
    "    purity = numerator/len(True_Label)\n",
    "    NMF_purity.append(purity)\n",
    "    \n",
    "    # AMI\n",
    "    AMI = adjusted_mutual_info_score(True_Label, estimate_NMF)\n",
    "    NMF_AMI.append(AMI)\n",
    "    \n",
    "    # ARI\n",
    "    ARI = adjusted_rand_score(True_Label, estimate_NMF)\n",
    "    NMF_ARI.append(ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "difficult-stranger",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cluster = []\n",
    "for i in range(0,68):\n",
    "    num_cluster.append(i+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "detected-tract",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_cluster</th>\n",
       "      <th>Purity</th>\n",
       "      <th>AMI</th>\n",
       "      <th>ARI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.506416</td>\n",
       "      <td>0.078822</td>\n",
       "      <td>0.110822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.546785</td>\n",
       "      <td>0.192524</td>\n",
       "      <td>0.193621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.563365</td>\n",
       "      <td>0.213600</td>\n",
       "      <td>0.259313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.550029</td>\n",
       "      <td>0.201867</td>\n",
       "      <td>0.243048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.563437</td>\n",
       "      <td>0.195759</td>\n",
       "      <td>0.176873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>66</td>\n",
       "      <td>0.668325</td>\n",
       "      <td>0.161217</td>\n",
       "      <td>0.028121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>67</td>\n",
       "      <td>0.683499</td>\n",
       "      <td>0.172003</td>\n",
       "      <td>0.033676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>68</td>\n",
       "      <td>0.671749</td>\n",
       "      <td>0.165080</td>\n",
       "      <td>0.032254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>69</td>\n",
       "      <td>0.678417</td>\n",
       "      <td>0.165137</td>\n",
       "      <td>0.029846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>70</td>\n",
       "      <td>0.671893</td>\n",
       "      <td>0.160323</td>\n",
       "      <td>0.026732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_cluster    Purity       AMI       ARI\n",
       "0             3  0.506416  0.078822  0.110822\n",
       "1             4  0.546785  0.192524  0.193621\n",
       "2             5  0.563365  0.213600  0.259313\n",
       "3             6  0.550029  0.201867  0.243048\n",
       "4             7  0.563437  0.195759  0.176873\n",
       "..          ...       ...       ...       ...\n",
       "63           66  0.668325  0.161217  0.028121\n",
       "64           67  0.683499  0.172003  0.033676\n",
       "65           68  0.671749  0.165080  0.032254\n",
       "66           69  0.678417  0.165137  0.029846\n",
       "67           70  0.671893  0.160323  0.026732\n",
       "\n",
       "[68 rows x 4 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NMF_evaluation_metrics = {'num_cluster': num_cluster,'Purity': NMF_purity, 'AMI': NMF_AMI, 'ARI': NMF_ARI}\n",
    "NMF_evaluation_metrics = pd.DataFrame(data = NMF_evaluation_metrics)\n",
    "NMF_evaluation_metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "possible-senate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF_evaluation_metrics.to_csv(\"/home/rep/scRNA-seq_clustering_to_Twitter/P2_scRNAseq_LDA_NMF/LDA_NMF/LDA_NMF_files/NMF_jobs_evaluation_metrics_raw_stemming.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
