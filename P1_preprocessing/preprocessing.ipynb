{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "difficult-garage",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/rep/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/rep/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Packages need for data pre-process\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "from scipy import sparse\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# word normalization (remove duplicate letters (e.g. llllooooovvvveee -> love))\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from repeatedReplacer import RepeatReplacer \n",
    "replacer = RepeatReplacer()\n",
    "\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "# read in functions from 'preprocessingFunctions.py'\n",
    "import preprocessingFunctions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-stack",
   "metadata": {},
   "source": [
    "### - Tweets from 4 distinct users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "broadband-mineral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12780, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of         user_id  user_id_new      screen_name  \\\n",
       "0      27902825            2    UMichFootball   \n",
       "1      27902825            2    UMichFootball   \n",
       "2      27902825            2    UMichFootball   \n",
       "3      27902825            2    UMichFootball   \n",
       "4      27902825            2    UMichFootball   \n",
       "...         ...          ...              ...   \n",
       "12775  19071682            3  breakingweather   \n",
       "12776  19071682            3  breakingweather   \n",
       "12777  19071682            3  breakingweather   \n",
       "12778  19071682            3  breakingweather   \n",
       "12779  19071682            3  breakingweather   \n",
       "\n",
       "                                                    text  \n",
       "0                              👇 https://t.co/swtsZWWaJe  \n",
       "1      Leave it all on the field! @UMichFootball! Bes...  \n",
       "2      There’s no time to look backwards… only ahead!...  \n",
       "3         2️⃣4️⃣:0️⃣0️⃣:0️⃣0️⃣ ⏳ https://t.co/eM3yUXJXaq  \n",
       "4      It’s called “The Game’ for a reason. \\r\\n\\r\\n#...  \n",
       "...                                                  ...  \n",
       "12775  A flash flood emergency is in effect for south...  \n",
       "12776  Now that Barry, the first hurricane to make U....  \n",
       "12777  Showers and locally heavy, drenching thunderst...  \n",
       "12778  While Monday felt like a typical summer day in...  \n",
       "12779  Residents from the northern Philippines to Tai...  \n",
       "\n",
       "[12780 rows x 4 columns]>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dataset\n",
    "import os \n",
    "os.chdir(\"/home/rep/scRNA-seq_clustering_to_Twitter/P1_preprocessing\")\n",
    "os.getcwd()\n",
    "four = pd.read_csv('four_users.csv')\n",
    "del four['Unnamed: 0']\n",
    "print(four.shape)\n",
    "four.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "anticipated-narrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lowercase and convert to list\n",
    "data = four.text.str.lower().values.tolist()\n",
    "data = [preprocessingFunctions.preProcessingFcn(tweet) for tweet in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "friendly-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the tweets and remove punctuations\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "closing-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "stop_words = stopwords.words('english')\n",
    "data_words_unigrams = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in data_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dramatic-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "data = []\n",
    "for i in data_words_unigrams:\n",
    "    tweet = ' '.join(i)\n",
    "    data.append(tweet)\n",
    "\n",
    "data_stemming = [preprocessingFunctions.stemming(tweet) for tweet in data]\n",
    "\n",
    "data_stemming_temp = []\n",
    "for i in data_stemming:\n",
    "    alist = i.split()\n",
    "    data_stemming_temp.append(alist)\n",
    "    \n",
    "data_stemming = data_stemming_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "geological-lighter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of remaining tweets w.r.t. original tweets: 97.86%\n",
      "Proportion of removed tweets w.r.t. original tweets: 2.14%\n"
     ]
    }
   ],
   "source": [
    "# Remove 80% of the least frequent words\n",
    "words_dict, data_stemming1, empty_idx = preprocessingFunctions.trim_noise(data_stemming, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "southern-survey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The lowest word frequency in the remaining tweets \n",
    "min(words_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "anticipated-pricing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12507, 2043)\n",
      "       beatosu  best  colleg  field  footbal  goblu  leav  rivalri  \\\n",
      "1          1.0   1.0     1.0    1.0      1.0    1.0   1.0      1.0   \n",
      "2          1.0   0.0     0.0    0.0      0.0    1.0   0.0      0.0   \n",
      "4          1.0   0.0     0.0    0.0      0.0    1.0   0.0      0.0   \n",
      "5          1.0   0.0     0.0    0.0      0.0    1.0   0.0      0.0   \n",
      "7          1.0   0.0     0.0    0.0      0.0    1.0   0.0      0.0   \n",
      "...        ...   ...     ...    ...      ...    ...   ...      ...   \n",
      "12775      0.0   0.0     0.0    0.0      0.0    0.0   0.0      0.0   \n",
      "12776      0.0   0.0     0.0    0.0      0.0    0.0   0.0      0.0   \n",
      "12777      0.0   0.0     0.0    0.0      0.0    0.0   0.0      0.0   \n",
      "12778      0.0   0.0     0.0    0.0      0.0    0.0   0.0      0.0   \n",
      "12779      0.0   0.0     0.0    0.0      0.0    0.0   0.0      0.0   \n",
      "\n",
      "       umichfootbal  ahead  ...  antil  lesser  uptick  dorian  \\\n",
      "1               1.0    0.0  ...    0.0     0.0     0.0     0.0   \n",
      "2               1.0    1.0  ...    0.0     0.0     0.0     0.0   \n",
      "4               0.0    0.0  ...    0.0     0.0     0.0     0.0   \n",
      "5               0.0    0.0  ...    0.0     0.0     0.0     0.0   \n",
      "7               1.0    0.0  ...    0.0     0.0     0.0     0.0   \n",
      "...             ...    ...  ...    ...     ...     ...     ...   \n",
      "12775           0.0    0.0  ...    0.0     0.0     0.0     0.0   \n",
      "12776           0.0    0.0  ...    0.0     0.0     0.0     0.0   \n",
      "12777           0.0    0.0  ...    0.0     0.0     0.0     0.0   \n",
      "12778           0.0    0.0  ...    0.0     0.0     0.0     0.0   \n",
      "12779           0.0    0.0  ...    0.0     0.0     0.0     0.0   \n",
      "\n",
      "       hurricanedorian  bailu  krosa  lekima  flossi  barri  \n",
      "1                  0.0    0.0    0.0     0.0     0.0    0.0  \n",
      "2                  0.0    0.0    0.0     0.0     0.0    0.0  \n",
      "4                  0.0    0.0    0.0     0.0     0.0    0.0  \n",
      "5                  0.0    0.0    0.0     0.0     0.0    0.0  \n",
      "7                  0.0    0.0    0.0     0.0     0.0    0.0  \n",
      "...                ...    ...    ...     ...     ...    ...  \n",
      "12775              0.0    0.0    0.0     0.0     0.0    0.0  \n",
      "12776              0.0    0.0    0.0     0.0     0.0    1.0  \n",
      "12777              0.0    0.0    0.0     0.0     0.0    1.0  \n",
      "12778              0.0    0.0    0.0     0.0     0.0    1.0  \n",
      "12779              0.0    0.0    0.0     0.0     0.0    0.0  \n",
      "\n",
      "[12507 rows x 2043 columns]\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "##### Create document-term matrix #####\n",
    "#######################################\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_stemming1)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_stemming1\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "a_s = gensim.matutils.corpus2dense(corpus, num_terms = len(words_dict))\n",
    "b_s = a_s.T.astype(np.float64)\n",
    "\n",
    "# Extract Document index\n",
    "selected_idex = [x for x in list(four.index) if x not in empty_idx]\n",
    "\n",
    "# Obtain remaining terms\n",
    "words = [] \n",
    "for i,j in enumerate(id2word):\n",
    "    words.append(id2word[i])\n",
    "\n",
    "# Create a dataframe for the document-term matrix\n",
    "b_ss = pd.DataFrame(b_s, columns=words, index=selected_idex)\n",
    "print(b_ss.shape)\n",
    "print(b_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "vital-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain attributes for the remaining tweet \n",
    "four_after = four.drop(empty_idx, axis=0)\n",
    "\n",
    "tweets_processed = []\n",
    "for i in data_stemming1:\n",
    "    tweet = ' '.join(i)\n",
    "    tweets_processed.append(tweet)\n",
    "\n",
    "four_after['tweets_processed'] = list(tweets_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b_ss.to_csv(\"doc_word_matrix_stemming_four_users.csv\")\n",
    "# four_after.to_csv(\"doc_metadata_stemming_four_users.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-satellite",
   "metadata": {},
   "source": [
    "### - \"jobs\" Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "portuguese-scottish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27900, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                       time                                               text  \\\n",
       "0      2009-08-01 10:25:36  Now Hiring:  Storage Architect II http://bit.l...   \n",
       "1      2009-08-01 22:57:06  \"The Steve Jobs method\" discussion on Hacker N...   \n",
       "2      2009-08-01 23:27:08  AZ Jobs | Taco Bell Restaurant General Manager...   \n",
       "3      2009-08-01 09:55:12  TN Jobs | SLP Travel Job in Knoxville Area, TN...   \n",
       "4      2009-08-01 05:58:39  NJ Jobs | New Jersey Travel or Perm job- OT at...   \n",
       "...                    ...                                                ...   \n",
       "27895  2009-11-01 02:15:14  these guys have to wake up. make him work alre...   \n",
       "27896  2009-11-01 03:04:26  Therapy Jobs at HCR! Physical Therapist / PT -...   \n",
       "27897  2009-11-01 00:21:24              hospitality jobs http://bit.ly/3XvUT1   \n",
       "27898  2009-11-01 03:26:41  Obama Tempers Economic News With Caution On Jo...   \n",
       "27899  2009-11-01 03:21:23  EXCITING, getting ready for my 1st job test =D...   \n",
       "\n",
       "                   sn        date  \n",
       "0       ChicagoJobAds  2009-08-01  \n",
       "1              hnshah  2009-08-01  \n",
       "2          ZuluJobsAZ  2009-08-01  \n",
       "3          ZuluJobsTN  2009-08-01  \n",
       "4          ZuluJobsNJ  2009-08-01  \n",
       "...               ...         ...  \n",
       "27895     yankee32879  2009-11-01  \n",
       "27896      lydsterj2w  2009-11-01  \n",
       "27897  Ur_WebInfoNews  2009-11-01  \n",
       "27898  suzanne_newton  2009-11-01  \n",
       "27899  NadaAbdulrazak  2009-11-01  \n",
       "\n",
       "[27900 rows x 4 columns]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dataset\n",
    "jobs = pd.read_csv(\"jobs_tweets_sampled_three_month.csv\", encoding= 'unicode_escape')\n",
    "del jobs['Unnamed: 0']\n",
    "\n",
    "print(jobs.shape)\n",
    "jobs.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "furnished-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lowercase and convert to list\n",
    "data = jobs.text.str.lower().values.tolist()\n",
    "data = [preprocessingFunctions.preProcessingFcn(tweet) for tweet in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "identical-reconstruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the tweets and remove punctuations\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "postal-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "stop_words = stopwords.words('english')\n",
    "data_words_unigrams = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in data_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "every-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "data = []\n",
    "for i in data_words_unigrams:\n",
    "    tweet = ' '.join(i)\n",
    "    data.append(tweet)\n",
    "\n",
    "data_stemming = [preprocessingFunctions.stemming(tweet) for tweet in data]\n",
    "\n",
    "data_stemming_temp = []\n",
    "for i in data_stemming:\n",
    "    alist = i.split()\n",
    "    data_stemming_temp.append(alist)\n",
    "    \n",
    "data_stemming = data_stemming_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "sunset-angola",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of remaining tweets w.r.t. original tweets: 99.99%\n",
      "Proportion of removed tweets w.r.t. original tweets: 0.01%\n"
     ]
    }
   ],
   "source": [
    "# Remove 90% of the least frequent words\n",
    "words_dict, data_stemming1, empty_idx1 = preprocessingFunctions.trim_noise(data_stemming, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "metric-phone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The lowest word frequency in the remaining tweets \n",
    "min(words_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "average-labor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282    http://bit.ly/rXYm5 :: e_jobs: &#10148;Concurs...\n",
      "Name: text, dtype: object\n",
      "13865    legitimate_telecommute_jobs  http://bit.ly/16tkOq\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print the removed tweets \n",
    "for i in empty_idx1:\n",
    "    print(jobs.iloc[[i]].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adequate-height",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27898, 2146)\n",
      "       architect  hire   ii  job  discuss  news  steve  via   az  azjob  ...  \\\n",
      "0            1.0   1.0  1.0  1.0      0.0   0.0    0.0  0.0  0.0    0.0  ...   \n",
      "1            0.0   0.0  0.0  1.0      1.0   1.0    1.0  1.0  0.0    0.0  ...   \n",
      "2            0.0   1.0  0.0  2.0      0.0   0.0    0.0  0.0  2.0    1.0  ...   \n",
      "3            0.0   1.0  0.0  3.0      0.0   0.0    0.0  0.0  0.0    0.0  ...   \n",
      "4            0.0   1.0  0.0  3.0      0.0   0.0    0.0  0.0  0.0    0.0  ...   \n",
      "...          ...   ...  ...  ...      ...   ...    ...  ...  ...    ...  ...   \n",
      "27895        0.0   0.0  0.0  1.0      0.0   0.0    0.0  0.0  0.0    0.0  ...   \n",
      "27896        0.0   0.0  0.0  2.0      0.0   0.0    0.0  0.0  0.0    0.0  ...   \n",
      "27897        0.0   0.0  0.0  1.0      0.0   0.0    0.0  0.0  0.0    0.0  ...   \n",
      "27898        0.0   0.0  0.0  1.0      0.0   2.0    0.0  0.0  0.0    0.0  ...   \n",
      "27899        0.0   0.0  0.0  2.0      0.0   0.0    0.0  0.0  0.0    0.0  ...   \n",
      "\n",
      "       airway  australian  halloween  nano  effort  oct  radioshack  iwow  \\\n",
      "0         0.0         0.0        0.0   0.0     0.0  0.0         0.0   0.0   \n",
      "1         0.0         0.0        0.0   0.0     0.0  0.0         0.0   0.0   \n",
      "2         0.0         0.0        0.0   0.0     0.0  0.0         0.0   0.0   \n",
      "3         0.0         0.0        0.0   0.0     0.0  0.0         0.0   0.0   \n",
      "4         0.0         0.0        0.0   0.0     0.0  0.0         0.0   0.0   \n",
      "...       ...         ...        ...   ...     ...  ...         ...   ...   \n",
      "27895     0.0         0.0        0.0   0.0     0.0  0.0         0.0   0.0   \n",
      "27896     0.0         0.0        0.0   0.0     0.0  0.0         0.0   0.0   \n",
      "27897     0.0         0.0        0.0   0.0     0.0  0.0         0.0   0.0   \n",
      "27898     0.0         0.0        0.0   0.0     0.0  0.0         0.0   0.0   \n",
      "27899     0.0         0.0        0.0   0.0     0.0  0.0         0.0   0.0   \n",
      "\n",
      "       persist  overst  \n",
      "0          0.0     0.0  \n",
      "1          0.0     0.0  \n",
      "2          0.0     0.0  \n",
      "3          0.0     0.0  \n",
      "4          0.0     0.0  \n",
      "...        ...     ...  \n",
      "27895      0.0     0.0  \n",
      "27896      0.0     0.0  \n",
      "27897      0.0     0.0  \n",
      "27898      0.0     0.0  \n",
      "27899      0.0     0.0  \n",
      "\n",
      "[27898 rows x 2146 columns]\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "##### Create document-term matrix #####\n",
    "#######################################\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_stemming1)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_stemming1\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "a_s = gensim.matutils.corpus2dense(corpus, num_terms = len(words_dict))\n",
    "b_s = a_s.T.astype(np.float64)\n",
    "\n",
    "# Extract Document index\n",
    "selected_idex = [x for x in list(jobs.index) if x not in empty_idx1]\n",
    "\n",
    "# Obtain remaining terms\n",
    "words = [] \n",
    "for i,j in enumerate(id2word):\n",
    "    words.append(id2word[i])\n",
    "\n",
    "# Create a dataframe\n",
    "b_ss = pd.DataFrame(b_s, columns=words, index=selected_idex)\n",
    "print(b_ss.shape)\n",
    "print(b_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "black-bowling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of remaining tweets w.r.t. original tweets: 99.45%\n",
      "Proportion of removed tweets w.r.t. original tweets: 0.55%\n",
      "(27744, 2145)\n"
     ]
    }
   ],
   "source": [
    "# Remove the words that appear in >= 80% of the tweets\n",
    "word_dict, data_stemming2, b_ss_f, empty_idx2 = preprocessingFunctions.trim_common(b_ss, 80, data_stemming1)\n",
    "print(b_ss_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adjustable-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the idex of all empty tweets after pre-processing\n",
    "empty_idx = empty_idx1 + empty_idx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "mexican-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain attributes for the remaining tweet \n",
    "jobs_after = jobs.drop(empty_idx, axis=0)\n",
    "\n",
    "tweets_processed = []\n",
    "for i in data_stemming2:\n",
    "    tweet = ' '.join(i)\n",
    "    tweets_processed.append(tweet)\n",
    "\n",
    "jobs_after['tweets_processed'] = list(tweets_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-spare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b_ss_f.to_csv(\"doc_word_matrix_stemming_jobs.csv\")\n",
    "# jobs_after.to_csv(\"doc_metadata_stemming_jobs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-price",
   "metadata": {},
   "source": [
    "# Ferg's distance matrix\n",
    "##### Reference: https://deepblue.lib.umich.edu/handle/2027.42/163193\n",
    "##### Ferg, Robyn. Modern Survey Estimation with Social Media and Auxiliary Data. Diss. 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "transparent-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "## packages\n",
    "import os \n",
    "import csv\n",
    "import numpy as np\n",
    "from pyclustering.cluster.kmedoids import kmedoids\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm # table\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "import random \n",
    "\n",
    "\n",
    "## first, read in functions from 'distanceMatrices.py'\n",
    "import distanceMatrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-guard",
   "metadata": {},
   "source": [
    "#### (1) tweets from four distinct users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-happiness",
   "metadata": {},
   "outputs": [],
   "source": [
    "minMentions = 0\n",
    "wordDistMethod = 'condProb'\n",
    "newMethod1 = distanceMatrices.makeMatrices(list(four_after.tweets_processed), minMentions=minMentions, preProcess=False, wordDistMethod=wordDistMethod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_distance_matrix = newMethod1['s']\n",
    "# tweet_distance_matrix = np.asmatrix(tweet_distance_matrix)\n",
    "# tweet_distance_matrix = pd.DataFrame(tweet_distance_matrix)\n",
    "# tweet_distance_matrix.to_csv(\"tweet_distance_matrix_four_users.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-clearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_distance_matrix = newMethod1['d']\n",
    "# word_distance_matrix = np.asmatrix(word_distance_matrix)\n",
    "# word_distance_matrix = pd.DataFrame(word_distance_matrix)\n",
    "# word_distance_matrix.to_csv(\"word_distance_matrix_four_users.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-yugoslavia",
   "metadata": {},
   "source": [
    "#### (2) \"jobs\" tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "palestinian-diabetes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating word matrix\n",
      "Making co-occurrence matrix\n",
      "Making word transition matrix\n",
      "Making word distance matrix\n",
      "Making tweet distance matrix\n"
     ]
    }
   ],
   "source": [
    "minMentions = 0\n",
    "wordDistMethod = 'condProb'\n",
    "newMethod = distanceMatrices.makeMatrices(list(jobs_after.tweets_processed), minMentions=minMentions, preProcess=False, wordDistMethod=wordDistMethod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "coordinated-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_distance_matrix = newMethod['s']\n",
    "# tweet_distance_matrix = np.asmatrix(tweet_distance_matrix)\n",
    "# tweet_distance_matrix = pd.DataFrame(tweet_distance_matrix)\n",
    "# tweet_distance_matrix.to_csv(\"tweet_distance_matrix_jobs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "detected-spoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_distance_matrix = newMethod['d']\n",
    "# word_distance_matrix = np.asmatrix(word_distance_matrix)\n",
    "# word_distance_matrix = pd.DataFrame(word_distance_matrix)\n",
    "# word_distance_matrix.to_csv(\"word_distance_matrix_jobs.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
